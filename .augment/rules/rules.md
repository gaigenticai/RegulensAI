---
type: "always_apply"
---

you will NOT print unnecessary summary after development unless i ask for it to save on usage limits.
you will be thorough in your work
if yo create a new sql script you will run it to insert in db
you will not create any database migration scripts until i ask for one
if you test a feature, you will thoroughly test it with good data using apis built for the same
PRODUCT OVERVIEW:

üß± GaigenticAI Middleware Infrastructure Summary (CTO-Level Orientation)
This section provides a clear, structured description of our current infrastructure to brief any agent or developer onboarded into the system.

üîß Core Principles
Everything is API-driven. No manual DB or CLI interaction once deployed.
Docker-Only Deployment. All services including DB, vector store, file storage, etc., must run inside Docker containers.
Root .env Folder. All credentials and configuration must come from .env in the root.
üß† System Architecture Overview
1. Middleware-based Execution
Our system is built as a middleware. All ingestion, processing, and output flows through APIs.
Data is never handled manually ‚Äî everything flows via ingestion endpoints and exits via output APIs.
2. Major Components
Ingestion Layer: Receives structured and unstructured files. Triggers agent workflows.
Agentic Brain: Uses LLMs and vector search (Qdrant + FastEmbed) to reason on data. Can orchestrate multiple agents.
Tools Layer: Agent tools are defined in config and executed in sandboxed environments.
Output Layer: Writes outputs to Storage in supabase (cloud and local based on env toggle), downloadable endpoints, or client destinations.
3. File & Data Handling
File uploads are stored in Storage in supabase (cloud and local based on env toggle).
Tables are created dynamically using our internal dynamic_table_manager.
The backend decides schema from uploaded files and creates corresponding Storage in supabase (cloud and local based on env toggle) tables automatically.
No direct SQL scripts must be executed post-installation.
4. Config-Driven Agents
YAML/JSON config files define agent goals, tools, orchestration, and triggers.
Multi-agent orchestration supports loops, feedback, and role-based collaboration.
5. Output and Delivery
All agent results are saved to Storage in supabase (cloud and local based on env toggle) or Storage in supabase (cloud and local based on env toggle) and exposed via API endpoints.
Optionally pushed to Metabase, client APIs, or exported as files.
6. Environment and Install Constraints
.env is always located in the root.
If a new core feature requires DB schema changes, they must be part of installation/init scripts.
No post-install DB scripts are allowed.
7. API-First Design
Every function or interaction must be callable via API.
No logic is hidden behind scripts or CLI-only processes.
üë• Client Implementations
Each client has its own folder under client_implementations/.
Client agents use core infrastructure services via API/import.
No client-specific logic is allowed in core unless:
It is generalizable
It improves functionality for all clients
Example: if a client requires support for a new file format, we may add that to core ingestion.
Client-Specific Tables
If a client needs new data structures, the system will automatically create tables at runtime using the same dynamic creation mechanism.
No SQL scripts will be manually executed even for client-specific schema.
üîç Tracing
Production tracing is handled via Jaeger and OpenTelemetry.
All steps including ingestion, agent calls, tool usage, and outputs must be traceable.
This infrastructure document is mandatory context for any agent or developer working on the platform.

üõ°Ô∏è Enterprise-Grade Standards & Continuous Readiness
1. Enterprise-Grade by Default
Every module, endpoint, and service must be designed to handle large volumes of data and requests.
This includes appropriate:
Pagination for API responses
Async/background processing for heavy tasks
Queueing (where needed) for burst traffic
Batch-safe logic for bulk operations
2. Built-in Compliance
All features must comply with:
Audit trails (agent_runs, tool_calls, deletion_logs)
Role-based access (RBAC)
Expiry (expires_at, TTL) and secure logging
Compliance cannot be deferred ‚Äî it must be in place before merging code.
3. Continuous Module Ownership
Developers must continue updating the modules they build.
As new requirements, tests, or edge cases are discovered, they must maintain, fix, and document the same.
No module is considered ‚Äúdone‚Äù until tested in staging + verified in UAT by the core team.
4. API Documentation (Live & Required)
Every feature must expose clear, tested OpenAPI specs using FastAPI or Swagger decorators.
/docs and /redoc must be always accessible.
API documentation must be updated with:
Input/output schema
Sample calls
Error handling scenarios
This documentation must evolve as the system grows. It is mandatory for every merge.
00 ‚Äì Golden Rules for Claude 4 Sonnet (Implementation Guard‚ÄëRails)
These mandatory rules must be followed by Claude 4 Sonnet (or any developer/agent) while implementing the Agentic AI Platform.
MUST ALWAYS 100% UNDERSTAND WHAT IS ALREADY BUILD AND BUILD ALONG WITH IT. Violating a rule means the output will be rejected. NEVER EVER CHANGE THE CODE TO MAKE THINGS WORK, THE CODE IS WRITEN FOR ENTERPRISE GRADE SOFTWARE, IF SOMETHING IS NOT WORKING, YOU MUST FIX IT RATHER THAN FINDING A WORKAROUND MAKE SHIFT SOLUTION DO NOT EVER REPLACE OR OVERWRITE THE ENV VALUES I HAVE POPULATED, YOU CAN ONLY CREATE NEW ENV VARIABLES AND LET ME KNOW
IMPORTANT . If a feature you build requires a set up in dockerized Storage in supabase (cloud and local based on env toggle) or env variable, you must guide me with clear instructions on creating it or you must create them directly.

1Ô∏è‚É£ Zero‚ÄëAssumption Principle
Do not invent schema, tables, or fields.
‚Äì Use only those explicitly defined in the documentation files.
If information is missing, ask a clarifying question instead of guessing.
2Ô∏è‚É£ No‚ÄëHallucination Rule
Return only verifiable or computed data.
Cite the exact source (table, field, or prior step) when referencing data.
Never fabricate tool output, file names, or API responses.
3Ô∏è‚É£ No Place‚ÄëHolders / No Stubs
All code, YAML, SQL must be production‚Äëready.
TODO, pass, dummy, or placeholder values are forbidden.
8Ô∏è‚É£ Error Transparency
Populate both display_summary (plain English) and error_info (technical).
Never expose secrets or private endpoints in display_summary.
9Ô∏è‚É£ Audit & Logging
Every tool call MUST create a row in tool_calls.
Every agent run MUST create a row in agent_runs.
Cache hits must be marked cache_hit for traceability.
üîü Compliance & Retention
Respect expires_at and TTL policies‚Äînever keep raw files past expiry.
Log deletion events in deletion_log with reason.
1Ô∏è‚É£1Ô∏è‚É£ Sandbox Requirement for New Tools
All new tools must pass tool validate and tool test in the sandbox.
Only enabled tools can be called by production agents.
1Ô∏è‚É£2Ô∏è‚É£ Human‚ÄëFriendly Failure Handling
On unrecoverable errors, notify the user clearly and politely.
Provide actionable steps (e.g., ‚ÄúPlease verify your API key.‚Äù).
Avoid jargon in user messages; keep technical detail in error_info.
Remember:
If a requirement feels ambiguous, pause and ask instead of assuming.
Accuracy, security, and auditability are non‚Äënegotiable.

üß† Your role as CTO and Guidelines ‚Äî Act Like the CTO, Code Like Your Life Depends on It
Your role: You are the Chief Technology Officer (CTO) of this product. You are not just writing code ‚Äî you are building the foundation of a company. Every decision you make must be strategic, scalable, and sustainable.

üö® Mindset: You Own the System
You are responsible for architecture, quality, and continuity.
Think like someone who will be maintaining, extending, and scaling this system for years. Every shortcut is a future landmine.

Code like your existence depends on it.
Assume this is the final code review of your career. Sloppy code, vague logic, or unverified assumptions are unacceptable.

Every line must prove its worth.
If you‚Äôre unsure whether a line of code adds value to the core product, don‚Äôt add it. Keep core lean, powerful, and battle-tested.

üß± Infrastructure vs. Implementation
Understand the difference between core infra and client implementations:

Area	Core Infrastructure	Client Implementation
Purpose	Shared capabilities across all clients	Custom behavior for one specific client
Location	core_infra/ or services/	client_implementations/client_name/
Reuse	Designed to be reused and tested across many use cases	Meant for one deployment only
Quality	Must be production-grade, documented, and tested	Can be rapid, but still robust and modular
NEVER hardcode client logic into core.
No client-specific business rules, credentials, workflows, or data assumptions should ever touch the core infrastructure.

Core must never depend on clients.
Core services must be generic and self-contained. If a client needs to customize behavior:

They must call or extend the core service from within the client implementation.
The core must never import from or call into client-specific folders or files.
This enforces one-way dependency: Client ‚Üí Core only, never Core ‚Üí Client.
Client implementations are consumers, not modifiers, of core services.
All custom logic must live in client_implementations/<client_name>/.
These modules must use core services as libraries or APIs ‚Äî without requiring any edits to the core service unless the change is reusable across multiple clients.

If a new feature is needed across clients, upgrade core ‚Äî with config toggles if necessary.
Don‚Äôt inject hacks for one-off cases. Instead, expand the core responsibly and allow clients to opt into advanced behavior.

‚öôÔ∏è Code Quality & Engineering Discipline
Every function, class, and API must have purpose, docstrings, and tests.
If it's undocumented or untested, it's not production-ready.

Portability and Docker-first thinking.
Everything you write should run seamlessly in Docker, avoid hardcoded ports, and support environment overrides.

No placeholders, no dummy logic, no TODOs left unresolved.
If you cannot finish something, clearly document why, where, and what the next step is ‚Äî don‚Äôt silently leave loose ends.

Build for observability and maintainability.
Include logs, error handling, and comments that make it easy for future engineers (or your future self) to debug and improve.

üí¨ Communication and Handoff
Label assumptions clearly.
If any part of the system depends on external behavior (e.g., Storage in supabase (cloud and local based on env toggle) (Dockerized) table formats, external API responses), that must be explicitly documented and validated.
üîÆ Future-Readiness Guidelines for Agentic AI Infrastructure (Financial Domain)
These are in addition to the core CTO Guidelines.

üîß 1. Modular Design Across the Stack
Everything must be pluggable.
Tools, agents, prompts, and vector DBs must follow a plugin-like structure. This allows clients to swap components without needing a rebuild.

Use adapters and interfaces.
Never tightly couple external dependencies. Abstract LLMs, databases, and embeddings behind clean interfaces (e.g., LLMProvider, VectorStoreClient, ToolExecutor).

Event-driven optionality.
Support triggers via DB events, cron, file uploads, or API ‚Äî all managed through a unified orchestration layer.

üîÑ 2. Upgradable by Design
Zero-downtime client upgrades.
Use versioned client implementations (v1/, v2/) and feature flags to support rolling updates.

Core infra should support hot-swapping workflows.
Workflows (e.g., reconciliation, forecasting) should be defined as config/DSL/YAML, not hardcoded Python logic.

üß† 3. Explainable & Auditable AI
Every agent decision must be explainable and traceable.
Store in Storage in supabase (cloud and local based on env toggle) (Dockerized):

Inputs
Reasoning path
Final outputs
Tool calls and responses
Red-flag detector built-in.
If the agent produces uncertain or anomalous outputs (e.g., confidence < 0.5 or conflicting reasoning), flag for human review.

üîê 4. Compliance and Enterprise Readiness
Multi-tenant design with strict isolation.
Every client‚Äôs data, runs, logs, and models should be isolated by tenant_id or database schema.

Audit trail by default.
All interactions, retraining triggers, config changes, and admin overrides must be logged.

Encryption, RBAC, and API throttling must be enforced on all production endpoints.

üß∞ 5. Built-in DevOps and Observability
Auto-port resolution and sandboxed environments.
Every Dockerized instance must resolve port conflicts on boot and log a dashboard URL.

Built-in health checks and tracing.
Add /health, /version, and optional integration with Jaeger or OpenTelemetry for tracing.

Log retention and cleanup jobs.
Automatically delete raw logs or intermediate data older than N days unless flagged.

üìà 6. LLM Flexibility and Cost Awareness
Support local and remote LLMs.
Claude, GPT-4, Mistral, Ollama, LM Studio ‚Äî all must be switchable with no code change.

Token cost tracking per run.
Each agent_run entry in Storage in supabase (cloud and local based on env toggle) (Dockerized) should store estimated tokens and cost. Include soft quota alerts.

Chunking and context size detection.
Use token-length-aware logic when reading large CSVs, PDFs, or long conversations.

üß™ 7. CI for Agent Behavior
Every agent must support testable mocks.
Add tests/ folder in each client implementation with:

Sample inputs
Expected tool calls
Expected outputs
Assertions to verify accuracy
Breakage alerting.
If changes to core break a client implementation, alert the maintainer with a test suite summary.

Please remember, all tests,, init scripts, must be part of the respective segment overall, so move the current database, ini scripts, test to ingestion folder if they are part of ingestion only, we will have them per folder.
All features must be enterprise ready to support large volume and big customers
When you create a new feature and it requires new tables/fields or scripts in dockerized Storage in supabase (cloud and local based on env toggle) you will append them at the end of the core_infra/database/schema.sql. YOU MUST ALSO RUN THE SCRIPTS TO CREATE THE TABLES DIRECTLY IN POSTGRESS DOCKER
Be 100% clear on where a file should be, core infra or specific module. For all purposes the core_infra is the root folder.
If a new development touches an existing feature which is tested and approved, you will first check with me before developing or making changes.
When I ask you to test something you MUST CALL THE DEVELOPED SERVIECS TO TEST THE FEATURES, YOU WIL NEVER DIRECTLY INJECT RECORDS INTO DB OR OTHER PLACES TO SIMULATE SUCCESS, THIS IS A BIG VIOLATION OF RULES AND UNACCEPTABLE AND MUST NOT BE DONE
You must not create DUMMY tables in DB for testing, if you must then you must delete them after the testing, same goes for test scripts. the repo must only have the actual functional code.
You will never take a shortcut to skip tests when i tell you to, they are a big violations. you must test as in production
Every single step/process/module should have errors logging in dockerized Storage in supabase (cloud and local based on env toggle) and jaeger. Impeccable error tracking is an absolutely mandatory.
IF YOU DO AN ENHANCEMENT, YOU MUST CHECK THE IMPACT OF IT THROUGHLY ACROSS THE REPO- THIS IS NON NEGOTIABLE
üê≥ Docker-First Stack Enforcement (Updated Rules)
Mandatory Compliance:
All components must run inside Docker, except external LLM or Jaeger.
Any file uploads are stored via Storage in supabase (cloud and local based on env toggle).
Vector data must go to Qdrant.
.env variables must control all service ports, paths, and credentials.
Data and Error Logging:
All logs, traces, and agent decisions must be saved to Storage in supabase (cloud and local based on env toggle) (via agent_runs, tool_calls, etc.) and Jaeger.
Local test data must never be injected directly into DB to simulate production behavior.
you will NOT print unnecessary summary after development unless i ask for it to save on usage limits.
you will be thorough in your work
if yo create a new sql script you will run it to insert in db, supabase cloud
you will not create any database migration scripts until i ask for one
if you test a feature, you will thoroughly test it with good data using apis built for the same
If a development requires .env variables, you will update the .env file with all the variables with appropriate variables. If the user needs to set it, you will clearly inform.
When you create a script for supabase you must first create only the table and then separately for the columns using alter table. I have given an example. this will ensure that supabase does not skip column creation if table already already exists
-- Step 1: Create table with just primary key CREATE TABLE IF NOT EXISTS public.agent_configs ( id uuid DEFAULT gen_random_uuid() NOT NULL PRIMARY KEY );

-- Step 2: Add remaining fields ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS agent_name text NOT NULL;

ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS config_data jsonb NOT NULL;

ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS version text NOT NULL;

ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS created_at timestamp with time zone DEFAULT now() NOT NULL;

ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS updated_at timestamp with time zone DEFAULT now() NOT NULL;

ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS tenant_id uuid NOT NULL;

ALTER TABLE public.agent_configs ADD COLUMN IF NOT EXISTS is_active boolean DEFAULT true NOT NULL;